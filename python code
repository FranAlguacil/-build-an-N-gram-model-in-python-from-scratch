
def preprocess_corpus(corpus_path):
    
    # Read in the text corpus from the file
    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus = f.read()

    # Preprocess the text corpus
    # Convert to lowercase
    corpus = corpus.lower()
    # Remove punctuation
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~'''
    for char in corpus:
        if char in punctuations:
            corpus = corpus.replace(char, "")
    # Split the corpus into sentences
    sentences = corpus.split('\n')
    # Remove any empty sentences
    sentences = [s for s in sentences if s]

    # Tokenize each sentence into a list of words
    tokenized_sentences = [s.split() for s in sentences]

    # Return the preprocessed corpus as a list of sentences
    return tokenized_sentences




def ngrams(sentences, n):
  
    # Initialize a list to store the n-grams
    ngrams = []
    
    # Iterate over each sentence in the corpus
    for sentence in sentences:
    
        # Generate the n-grams for the current sentence
        for i in range(len(sentence) - n + 1):
            ngram = sentence[i:i+n]
            ngrams.append(ngram)
   
    return ngrams



# This fuction is to generate as much n-grams as you can

sentences = preprocess_corpus('name of the corpus')
ngrams = []
for i in range(1, 6):
    ngrams.extend(generate_ngrams(sentences, i))

