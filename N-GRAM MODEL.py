# -*- coding: utf-8 -*-
"""syntax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tJ6AmT9JJCc0K7JJlqmxt5iKse4XD_GZ

# BUILD A N-GRAM MODEL

This code defines a function called preprocess_corpus that takes a path to a text corpus file as input. The function reads in the corpus file, converts all the text to lowercase, removes punctuations from the text, splits the text into sentences, removes any empty sentences, and tokenizes each sentence into a list of words. Finally, the function returns a list of lists, where each inner list represents a tokenized sentence from the original text corpus.
"""

#read the corpus, preprocess
def preprocess_corpus(corpus_path):
    
    corpus_path = "/content/slwiki-20230401-abstract.xml"
    # Read the corpus
    with open(corpus_path, 'r', encoding='utf-8') as f:
        corpus = f.read()

    # Preprocess the text corpus
    corpus = corpus.lower() #lowercase
    # Remove punctuations
    punctuations = '''!()-[]{};:'"\,<>./?@#$%^&*_~Â¬'''
    for char in corpus:
        if char in punctuations:
            corpus = corpus.replace(char, "")
    # Split into sentences
    sentences = corpus.split('\n')
    # Remove empty sentences
    sentences = [s for s in sentences if s]

    # Tokenize each sentence into a list of words
    tokenized_sentences = [s.split() for s in sentences]

   
    return tokenized_sentences

"""This code defines a function called ngrams that takes in a list of tokenized sentences and an integer n as input. The function generates n-grams for each sentence by iterating over every sentence and generating all possible contiguous subsequences of length n from that sentence. Each n-gram is appended to a list called ngrams, and this list of all n-grams is returned from the function. The second part of the code generates all possible n-grams for the given corpus, for values of n ranging from 1 to 5, using the ngrams function. The extend method is used to add the n-grams generated for each value of n to the same list ngrams. Finally, the resulting list of all n-grams is assigned to the variable ngrams. Note that it is not advisable to name a list variable with the same name as a function variable, as it can cause confusion and unexpected errors."""

#n-gram function

def ngrams(sentences, n):
  
    # let's create a list to save
    ngrams = []
    
    # Iteration
    for sentence in sentences:
    
        # Generate the n-grams for the current sentence
        for i in range(len(sentence) - n + 1):
            ngram = sentence[i:i+n]
            ngrams.append(ngram)
   
    return ngrams



# Generate as much n-grams as you can

sentences = preprocess_corpus('/content/slwiki-20230401-abstract.xml')
ngrams = []
for i in range(1, 6):
    ngrams.extend(generate_ngrams(sentences, i))

"""In  __init__(self, n) the parameter **n** represents the length of the n-grams, **self.ngrams** is a dictionary which will store the n-grams as keys and their frequency as values, and **self.total_count** represents the total number of n-grams in the corpus. The **train(self, corpus)** method tokenizes the input corpus, creates n-grams of length n, and stores their frequency in the self.ngrams dictionary. It also increments the **self.total_count** variable by the total number of n-grams. The **generation(self, sentence)**  tokenizes the input sentence, creates n-grams of length n, and calculates the log probability of the sentence using the n-gram model. The probability of each n-gram is estimated as its frequency divided by the total count of n-grams. The log probabilities of individual n-grams are added and returned as the log probability of the sentence. The **evaluation(self, test_corpus)**  tokenizes the test_corpus, creates n-grams of length n, and calculates the perplexity of the test corpus to measurehow well the n-gram model predicts the test corpus.



"""

class Ngram_model:
    def __init__(self, n):
        self.n = n
        self.ngrams = {}
        self.total_count = 0

    def train(self, corpus):
        # tokenize the corpus and create N-grams
        tokens = corpus.split()
        ngrams = create_ngrams(tokens, self.n)

        # frequency of each N-gram
        for ng in ngrams:
            if ng in self.ngrams:
                self.ngrams[ng] += 1
            else:
                self.ngrams[ng] = 1
            self.total_count += 1

    def generation(self, sentence):
        # tokenize and create N-grams
        tokens = sentence.split()
        ngrams = create_ngrams(tokens, self.n)

        # log probability of sentece
        log_prob = 0
        for ng in ngrams:
            count = self.ngrams.get(ng, 0) + 1  # one smoothing
            prob = count / (self.total_count + len(self.ngrams))
            log_prob += math.log(prob)

        return log_prob

    def evaluation(self, test_corpus):
        # tokenize the corpus and create N-grams
        tokens = test_corpus.split()
        ngrams = self.create_ngrams(tokens)

        # perplexity of the test corpus
        log_prob = 0
        for ng in ngrams:
            count = self.ngrams.get(ng, 0) + 1  # one smoothing
            prob = count / (self.total_count + len(self.ngrams))
            log_prob += math.log(prob)
        log_prob /= len(ngrams)
        perplexity = math.exp(-log_prob)

        return perplexity
    
    
    UPDATE! 
    
    
import nltk       #useful to treat with language model
import numpy as np   #operations with logs

nltk.download('brown')
corpus = nltk.corpus.brown
sentences = corpus.sents(categories='news')
tokenized_sentences = [[w.lower() for w in sent] for sent in sentences]


class NgramModel:
    def __init__(self, n, smoothing=0.01):      #Try to use MLE and Vocabulary from nltk.lm but didn't work 
        self.n = n
        self.vocab = set()
        self.ngram_counts = {}
        self.context_counts = {}
        self.smoothing = smoothing  
       
    def train(self, sentences):  #Define and train an N-gram
        for sent in sentences:
            padded_sent = ['<s>'] * self.n + sent + ['</s>']
            for i in range(self.n, len(padded_sent)):
                ngram = tuple(padded_sent[i - self.n:i])
                context = tuple(padded_sent[i - self.n:i - 1])  #fixed the error: : 'list' object has no attribute 'lookup' solucionalo
                self.vocab.add(padded_sent[i])
                if ngram in self.ngram_counts:
                    self.ngram_counts[ngram] += 1
                else:
                    self.ngram_counts[ngram] = 1
                if context in self.context_counts:
                    self.context_counts[context] += 1
                else:
                    self.context_counts[context] = 1

    def prob(self, word, context):
        context_count = self.context_counts.get(context, 0)
        ngram = context + (word,)
        ngram_count = self.ngram_counts.get(ngram, 0)
        prob = (ngram_count + self.smoothing) / (context_count + self.smoothing * len(self.vocab))
        return prob

    def sentence_logP_score(self, sentence):
        padded_sent = ['<s>'] * self.n + sentence + ['</s>']
        logP = 0
        for i in range(self.n, len(padded_sent)):
            ngram = tuple(padded_sent[i - self.n:i])
            context = tuple(padded_sent[i - self.n:i - 1])
            logP += np.log2(self.prob(padded_sent[i], context))
        return logP

    def generate(self, n_words=10):
        sentence = ['<s>'] * self.n
        for i in range(n_words):
            context = tuple(sentence[-self.n + 1:])    #Fixed: generate() got an unexpected keyword argument 'context'
            words = list(self.vocab)
            probs = [self.prob(w, context) for w in words]
            max_prob = max(probs)
            candidates = [word for word, prob in zip(words, probs) if prob == max_prob]
            sentence.append(candidates[0])
        return sentence[self.n:]

    def perplexity(self, sentence):  #Evaluation
        prob_sentence = self.sentence_logP_score(sentence)
        num_words = len(sentence)
        perplexity = 2 ** (-prob_sentence / num_words)
        return perplexity

    def sentence_interpolated_logP(S, vocab, uni_counts, bi_counts, tri_counts, lambdas=[0.5, 0.3, 0.2], alpha=1):
      tokens = ['*', '*'] + S + ['STOP']
      prob = 0
      for u, v, w in nltk.ngrams(tokens, 3):
        uni_prob = np.log(uni_counts[u] / sum(uni_counts.values()))
        bi_prob = np.log(bi_counts[(u, v)] / uni_counts[u])
        tri_prob = np.log(tri_counts[(u, v, w)] / bi_counts[(u, v)])
        prob += lambdas[0] * tri_prob + lambdas[1] * bi_prob + lambdas[2] * uni_prob
        return prob
