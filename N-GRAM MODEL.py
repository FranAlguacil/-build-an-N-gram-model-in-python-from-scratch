# -*- coding: utf-8 -*-
"""syntax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tJ6AmT9JJCc0K7JJlqmxt5iKse4XD_GZ

# BUILD A N-GRAM MODEL

This code defines a function called preprocess_corpus that takes a path to a text corpus file as input. The function reads in the corpus file, converts all the text to lowercase, removes punctuations from the text, splits the text into sentences, removes any empty sentences, and tokenizes each sentence into a list of words. Finally, the function returns a list of lists, where each inner list represents a tokenized sentence from the original text corpus.
"""    
import nltk       #useful to treat with language model
import numpy as np   #operations with logs

nltk.download('brown')
corpus = nltk.corpus.brown
sentences = corpus.sents(categories='news')
tokenized_sentences = [[w.lower() for w in sent] for sent in sentences]


class NgramModel:
    def __init__(self, n, smoothing=0.01):      #Try to use MLE and Vocabulary from nltk.lm but didn't work 
        self.n = n                              #objects
        self.vocab = set()
        self.ngram_counts = {}
        self.context_counts = {}
        self.smoothing = smoothing  
       
    def train(self, sentences):                #train ngram model
        for sent in sentences:
            padded_sent = ['<s>'] * self.n + sent + ['</s>']
            for i in range(self.n, len(padded_sent)):
                ngram = tuple(padded_sent[i - self.n:i])
                context = tuple(padded_sent[i - self.n:i - 1])
                self.vocab.add(padded_sent[i])
                self.ngram_counts[ngram] = self.ngram_counts.get(ngram, 0) + 1
                self.context_counts[context] = self.context_counts.get(context, 0) + 1

    def prob(self, word, context):             #probability + smoothing Laplace
        context_count = self.context_counts.get(context, 0)
        ngram = context + (word,)
        ngram_count = self.ngram_counts.get(ngram, 0)
        prob = (ngram_count + self.smoothing) / (context_count + self.smoothing * len(self.vocab))
        return prob

    def sentence_logP_score(self, sentence):   #score for the generate text
        padded_sent = ['<s>'] * self.n + sentence + ['</s>']
        logP = 0
        for i in range(self.n, len(padded_sent)):
            ngram = tuple(padded_sent[i - self.n:i])
            context = tuple(padded_sent[i - self.n:i - 1])
            logP += np.log2(self.prob(padded_sent[i], context))
        return logP

    def generate(self, n_words=10):            #generation 
        sentence = ['<s>'] * self.n
        for i in range(n_words):
            context = tuple(sentence[-self.n + 1:])    #Fixed: generate() got an unexpected keyword argument 'context'
            words = list(self.vocab)
            probs = [self.prob(w, context) for w in words]
            max_prob = max(probs)
            candidates = [word for word, prob in zip(words, probs) if prob == max_prob]
            sentence.append(candidates[0])
        return sentence[self.n:]

    def perplexity(self, sentence):  #Evaluation
        prob_sentence = self.sentence_logP_score(sentence)
        num_words = len(sentence)
        perplexity = 2 ** (-prob_sentence / num_words)
        return perplexity

    def sentence_interpolated_logP(S, vocab, uni_counts, bi_counts, tri_counts, lambdas=[0.5, 0.3, 0.2], alpha=1): #interpolation
      tokens = ['*', '*'] + S + ['STOP']
      prob = 0
      for u, v, w in nltk.ngrams(tokens, 3):
        uni_prob = np.log(uni_counts[u] / sum(uni_counts.values()))
        bi_prob = np.log(bi_counts[(u, v)] / uni_counts[u])
        tri_prob = np.log(tri_counts[(u, v, w)] / bi_counts[(u, v)])
        prob += lambdas[0] * tri_prob + lambdas[1] * bi_prob + lambdas[2] * uni_prob
        return prob

#Analysis part: I tried to define everything before in order to when we arrive to the experiment part, we only have to manage these three values below.
n = 5
smoothing = 0.01
discount = 0.05

model = NgramModel(n, smoothing)
model.train(tokenized_sentences)

generated_text = model.generate(20) #we can introduce the number of token we want
generated_text_list = list(generated_text)
print('Generated text:', ' '.join(generated_text_list))

generated_text_score = model.sentence_logP_score(generated_text)
print('Score of generated text:', generated_text_score)

generated_text_perplexity = model.perplexity(generated_text)
print('Perplexity of generated text:', generated_text_perplexity)

test_sentence = ['the', 'news', 'article', 'reported', 'on', 'the', 'recent', 'economic', 'trends']
test_sentence_score = model.sentence_logP_score(test_sentence)
print('Score of test sentence "', ' '.join(test_sentence), '":', test_sentence_score)

test_sentence_perplexity = model.perplexity(test_sentence)
print('Perplexity of test sentence "', ' '.join(test_sentence), '":', test_sentence_perplexity)

#Interpolation 
vocab = model.vocab
uni_counts = model.context_counts
bi_counts = model.ngram_counts
tri_counts = {}
lambdas = [0.5, 0.3, 0.2]

result = model.sentence_interpolated_logP(test_sentence, lambdas)
print('Interpolated log probability of test sentence "', ' '.join(test_sentence), '":', result)
